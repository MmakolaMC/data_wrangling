{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I manually downloaded the `twitter-archive-enhanced.csv` file and saved to my local environment, and then used the `requests` library to download the contents of the `image-predictions.tsv` flat file, which contained information and meta data about most of the dogs from the above mentioned csv file. which I then progammatically wrote to `image-predictions.tsv` on my local machine.\n",
    "\n",
    "2. I applied for elevated access on my Twitter developer account so I can query the API and get more information about how the public engaged with the said tweets. I was only able to aquire meta data for 2327 tweets in total from the Twitter API out of the 2356 I initially queried. I decided to use the new version of the API(v2), as the methods it offered are more faster and can aggregate data better than the old version(v1). The methods allowed me to query only what I required by passing fields of the tweets I was interested in and by doing so, I was able to speed up my code execution time and I did not have to create a file to read into a dataframe again saving on memory usage.\n",
    "\n",
    "3. I then assessed the csv and tsv files in a spreadsheed application on my local machine and noted any irregularities. These irregularities involved tidiness of the data like columns(doggo, puppo, pupper, floofer) which I in turn transformed into observations instead of units. I then continued to assess these said datasets after reading them into a jupyter notebook using pandas. I assesed and cleaned both the data from the csv and tsv files. I then joined the data from the Twitter API with the data from the `twitter-archive-enhaced.csv` file as i used the tweet ids from the file to query the data from the Twitter API.\n",
    "\n",
    "4. Finally I joined all the dataframes after cleaning and created one master dataset which I saved into `twitter_archive_master.csv` file. After joining these datasets I had in total 2057 observations from the 2356 I had started with.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
